{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "import models\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from /home/jake_miller/final/nlp-final-project/data\n",
      "Begin corpus post-processing ...\n",
      "Splitting corpus into training and test ...\n",
      "Creating vocabulary from training set ...\n",
      "Found 8435 unique words.\n",
      "Building initial embedding matrix ...\n",
      "(8437, 300)\n",
      "loading pretrained vectors from glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "# load the corpus\n",
    "corpus = utils.AMI_Corpus(seed = 75, embed_vec = 'glove300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Up: LSTM-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_model():\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # choose hyperparameters\n",
    "    num_layers = 3 \n",
    "    hidden_state_size = 150\n",
    "    dropout_rate = 0.4\n",
    "    bidirectional = True\n",
    "    stateful = False\n",
    "    trainable_embed = True\n",
    "\n",
    "    lstm = models.LSTMSoftmax(corpus,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              num_layers = num_layers,\n",
    "                              dropout_rate = dropout_rate,\n",
    "                              hidden_state_size = hidden_state_size,\n",
    "                              stateful = stateful, bidirectional = bidirectional, trainable_embed = trainable_embed)\n",
    "\n",
    "    lstm.model.compile(optimizer = 'adagrad', metrics = ['acc'], loss = 'categorical_crossentropy')\n",
    "\n",
    "    # create our generators\n",
    "    ug_train = utils.UtteranceGenerator(corpus, \"train\", batch_size = BATCH_SIZE, algo = \"LSTM_Soft\")\n",
    "    ug_val = utils.UtteranceGenerator(corpus, \"val\", batch_size = BATCH_SIZE, algo = \"LSTM_Soft\")\n",
    "    ug_test = utils.UtteranceGenerator(corpus, \"test\", batch_size = BATCH_SIZE, algo = \"LSTM_Soft\")\n",
    "\n",
    "    # create keras callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0, restore_best_weights = True)\n",
    "\n",
    "    train_start = time()\n",
    "    \n",
    "    # note to self, maybe change validation_steps and validation_freq\n",
    "    history = lstm.model.fit_generator(ug_train, epochs=EPOCHS, verbose=1, callbacks = [es], validation_data=ug_val, \n",
    "                                       use_multiprocessing=False, shuffle=True)\n",
    "    train_end = time()\n",
    "    \n",
    "    results = lstm.model.evaluate_generator(ug_test)\n",
    "    \n",
    "    test_end = time()\n",
    "    \n",
    "\n",
    "    return [(train_end - train_start), (test_end - train_end)] + results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 1.4419 - acc: 0.5511\n",
      "156/156 [==============================] - 11s 70ms/step - loss: 1.7047 - acc: 0.4397 - val_loss: 1.4419 - val_acc: 0.5511\n",
      "Epoch 2/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.3003 - acc: 0.5742\n",
      "156/156 [==============================] - 10s 62ms/step - loss: 1.3545 - acc: 0.5601 - val_loss: 1.3003 - val_acc: 0.5742\n",
      "Epoch 3/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.2255 - acc: 0.5823\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.2519 - acc: 0.5831 - val_loss: 1.2255 - val_acc: 0.5823\n",
      "Epoch 4/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.1798 - acc: 0.6107\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1912 - acc: 0.6046 - val_loss: 1.1798 - val_acc: 0.6107\n",
      "Epoch 5/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.1334 - acc: 0.6276\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1428 - acc: 0.6201 - val_loss: 1.1334 - val_acc: 0.6276\n",
      "Epoch 6/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.1072 - acc: 0.6288\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.1036 - acc: 0.6305 - val_loss: 1.1072 - val_acc: 0.6288\n",
      "Epoch 7/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0811 - acc: 0.6376\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0690 - acc: 0.6406 - val_loss: 1.0811 - val_acc: 0.6376\n",
      "Epoch 8/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0654 - acc: 0.6440\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 1.0426 - acc: 0.6460 - val_loss: 1.0654 - val_acc: 0.6440\n",
      "Epoch 9/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0629 - acc: 0.6410\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0227 - acc: 0.6519 - val_loss: 1.0629 - val_acc: 0.6410\n",
      "Epoch 10/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0673 - acc: 0.6423\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 1.0060 - acc: 0.6575 - val_loss: 1.0673 - val_acc: 0.6423\n",
      "Epoch 11/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0555 - acc: 0.6458\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9924 - acc: 0.6616 - val_loss: 1.0555 - val_acc: 0.6458\n",
      "Epoch 12/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0418 - acc: 0.6485\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9778 - acc: 0.6640 - val_loss: 1.0418 - val_acc: 0.6485\n",
      "Epoch 13/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0401 - acc: 0.6428\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9647 - acc: 0.6670 - val_loss: 1.0401 - val_acc: 0.6428\n",
      "Epoch 14/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0468 - acc: 0.6448\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9562 - acc: 0.6726 - val_loss: 1.0468 - val_acc: 0.6448\n",
      "Epoch 15/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0431 - acc: 0.6416\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9451 - acc: 0.6745 - val_loss: 1.0431 - val_acc: 0.6416\n",
      "Epoch 16/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0569 - acc: 0.6329\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9377 - acc: 0.6765 - val_loss: 1.0569 - val_acc: 0.6329\n",
      "Epoch 17/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0416 - acc: 0.6475\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9302 - acc: 0.6786 - val_loss: 1.0416 - val_acc: 0.6475\n",
      "Epoch 18/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0340 - acc: 0.6444\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9226 - acc: 0.6818 - val_loss: 1.0340 - val_acc: 0.6444\n",
      "Epoch 19/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0354 - acc: 0.6462\n",
      "156/156 [==============================] - 9s 61ms/step - loss: 0.9118 - acc: 0.6847 - val_loss: 1.0354 - val_acc: 0.6462\n",
      "Epoch 20/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0425 - acc: 0.6430\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.9052 - acc: 0.6870 - val_loss: 1.0425 - val_acc: 0.6430\n",
      "Epoch 21/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0390 - acc: 0.6426\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8967 - acc: 0.6885 - val_loss: 1.0390 - val_acc: 0.6426\n",
      "Epoch 22/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0489 - acc: 0.6406\n",
      "156/156 [==============================] - 10s 61ms/step - loss: 0.8896 - acc: 0.6917 - val_loss: 1.0489 - val_acc: 0.6406\n",
      "Epoch 23/25\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 1.0391 - acc: 0.6465\n",
      "156/156 [==============================] - 10s 63ms/step - loss: 0.8865 - acc: 0.6932 - val_loss: 1.0391 - val_acc: 0.6465\n"
     ]
    }
   ],
   "source": [
    "lstm_results = run_lstm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_model():\n",
    "    \n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # choose hyperparameters\n",
    "    d1 = 0\n",
    "    d2 = 2\n",
    "    filters = 200\n",
    "    kernel_size = 3\n",
    "    hidden_units = 140\n",
    "    dropout_rate = 0.5\n",
    "    trainable_embed = False\n",
    "    embed_vec = 'glove300'\n",
    "\n",
    "    cnn = models.CNN(corpus = corpus, d1 = d1, d2 = d2,\n",
    "                       batch_size = BATCH_SIZE,\n",
    "                       filters = filters,\n",
    "                       kernel_size = kernel_size,\n",
    "                       hidden_units = hidden_units,\n",
    "                       dropout_rate = dropout_rate,\n",
    "                       trainable_embed = trainable_embed)\n",
    "\n",
    "    cnn.model.compile(optimizer = 'adagrad', metrics = ['acc'], loss = 'categorical_crossentropy')\n",
    "\n",
    "    # create our generators\n",
    "    ug_train = utils.UtteranceGenerator(corpus, \"train\", batch_size = BATCH_SIZE, algo = \"CNN\", sequence_length = 3)\n",
    "    ug_val = utils.UtteranceGenerator(corpus, \"val\", batch_size = BATCH_SIZE, algo = \"CNN\", sequence_length = 3)\n",
    "    ug_test = utils.UtteranceGenerator(corpus, \"test\", batch_size = BATCH_SIZE, algo = \"CNN\", sequence_length = 3)\n",
    "\n",
    "    # create keras callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0, restore_best_weights = True)\n",
    "\n",
    "    train_start = time()\n",
    "    \n",
    "    # note to self, maybe change validation_steps and validation_freq\n",
    "    history = cnn.model.fit_generator(ug_train, epochs=EPOCHS, verbose=1, callbacks = [es], validation_data=ug_val, \n",
    "                                       use_multiprocessing=False, shuffle=True)\n",
    "    train_end = time()\n",
    "    \n",
    "    results = cnn.model.evaluate_generator(ug_test)\n",
    "    \n",
    "    test_end = time()\n",
    "    \n",
    "    return [(train_end - train_start), (test_end - train_end)] + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 1.3552 - acc: 0.5660\n",
      "156/156 [==============================] - 4s 29ms/step - loss: 3.5508 - acc: 0.4418 - val_loss: 1.3552 - val_acc: 0.5660\n",
      "Epoch 2/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.3040 - acc: 0.5644\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.3558 - acc: 0.5507 - val_loss: 1.3040 - val_acc: 0.5644\n",
      "Epoch 3/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.2298 - acc: 0.5891\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.2837 - acc: 0.5698 - val_loss: 1.2298 - val_acc: 0.5891\n",
      "Epoch 4/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.2042 - acc: 0.5990\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.2351 - acc: 0.5839 - val_loss: 1.2042 - val_acc: 0.5990\n",
      "Epoch 5/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1695 - acc: 0.6125\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.1943 - acc: 0.5983 - val_loss: 1.1695 - val_acc: 0.6125\n",
      "Epoch 6/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1388 - acc: 0.6171\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.1684 - acc: 0.6083 - val_loss: 1.1388 - val_acc: 0.6171\n",
      "Epoch 7/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1244 - acc: 0.6243\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.1402 - acc: 0.6142 - val_loss: 1.1244 - val_acc: 0.6243\n",
      "Epoch 8/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1313 - acc: 0.6165\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.1215 - acc: 0.6207 - val_loss: 1.1313 - val_acc: 0.6165\n",
      "Epoch 9/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1069 - acc: 0.6280\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.1051 - acc: 0.6279 - val_loss: 1.1069 - val_acc: 0.6280\n",
      "Epoch 10/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0924 - acc: 0.6277\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.0859 - acc: 0.6327 - val_loss: 1.0924 - val_acc: 0.6277\n",
      "Epoch 11/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0900 - acc: 0.6326\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.0744 - acc: 0.6377 - val_loss: 1.0900 - val_acc: 0.6326\n",
      "Epoch 12/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.1014 - acc: 0.6228\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.0604 - acc: 0.6405 - val_loss: 1.1014 - val_acc: 0.6228\n",
      "Epoch 13/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0883 - acc: 0.6250\n",
      "156/156 [==============================] - 3s 20ms/step - loss: 1.0446 - acc: 0.6462 - val_loss: 1.0883 - val_acc: 0.6250\n",
      "Epoch 14/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0792 - acc: 0.6278\n",
      "156/156 [==============================] - 3s 20ms/step - loss: 1.0320 - acc: 0.6508 - val_loss: 1.0792 - val_acc: 0.6278\n",
      "Epoch 15/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0876 - acc: 0.6279\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.0224 - acc: 0.6528 - val_loss: 1.0876 - val_acc: 0.6279\n",
      "Epoch 16/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0757 - acc: 0.6365\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.0095 - acc: 0.6607 - val_loss: 1.0757 - val_acc: 0.6365\n",
      "Epoch 17/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0749 - acc: 0.6300\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 0.9994 - acc: 0.6627 - val_loss: 1.0749 - val_acc: 0.6300\n",
      "Epoch 18/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.0863 - acc: 0.6303\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 0.9868 - acc: 0.6662 - val_loss: 1.0863 - val_acc: 0.6303\n",
      "Epoch 19/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0820 - acc: 0.6271\n",
      "156/156 [==============================] - 3s 20ms/step - loss: 0.9749 - acc: 0.6707 - val_loss: 1.0820 - val_acc: 0.6271\n",
      "Epoch 20/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0799 - acc: 0.6299\n",
      "156/156 [==============================] - 3s 20ms/step - loss: 0.9639 - acc: 0.6738 - val_loss: 1.0799 - val_acc: 0.6299\n",
      "Epoch 21/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0807 - acc: 0.6298\n",
      "156/156 [==============================] - 3s 20ms/step - loss: 0.9559 - acc: 0.6762 - val_loss: 1.0807 - val_acc: 0.6298\n",
      "Epoch 22/25\n",
      "26/26 [==============================] - 0s 9ms/step - loss: 1.0829 - acc: 0.6304\n",
      "156/156 [==============================] - 4s 23ms/step - loss: 0.9457 - acc: 0.6799 - val_loss: 1.0829 - val_acc: 0.6304\n"
     ]
    }
   ],
   "source": [
    "cnn_results = run_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3, the LSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstmcrf_model():\n",
    "    \n",
    "    from models_crf import BiLSTMCRF, UtteranceGenerator\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # choose hyperparameters\n",
    "    num_layers = 1\n",
    "    hidden_state_size = 300\n",
    "    dropout_rate = 0.4\n",
    "    bidirectional = True\n",
    "    trainable_embed = True\n",
    "    embed_vec = 'glove300'\n",
    "    sequence_length = 7\n",
    "\n",
    "    lstm_crf = BiLSTMCRF(corpus, batch_size = BATCH_SIZE,\n",
    "                         sequence_length = sequence_length,\n",
    "                         num_layers = num_layers,\n",
    "                         dropout_rate = dropout_rate,\n",
    "                         hidden_state_size = hidden_state_size,\n",
    "                         bidirectional = bidirectional, \n",
    "                         trainable_embed = trainable_embed)\n",
    "\n",
    "    lstm_crf.compile()\n",
    "\n",
    "    # create our generators\n",
    "    ug_train = UtteranceGenerator(corpus, \"train\", batch_size = BATCH_SIZE, algo = \"LSTM_CRF\", sequence_length = sequence_length)\n",
    "    ug_val = UtteranceGenerator(corpus, \"val\", batch_size = BATCH_SIZE, algo = \"LSTM_CRF\", sequence_length = sequence_length)\n",
    "    ug_test = UtteranceGenerator(corpus, \"test\", batch_size = BATCH_SIZE, algo = \"LSTM_CRF\", sequence_length = sequence_length)\n",
    "\n",
    "    # create keras callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0, restore_best_weights = True)\n",
    "\n",
    "    train_start = time()\n",
    "    \n",
    "    # note to self, maybe change validation_steps and validation_freq\n",
    "    history = lstm_crf.model.fit_generator(ug_train, epochs=EPOCHS, verbose=1, callbacks = [es], validation_data=ug_val, \n",
    "                                       use_multiprocessing=False, shuffle=True)\n",
    "    train_end = time()\n",
    "    \n",
    "    results = lstm_crf.model.evaluate_generator(ug_test)\n",
    "    \n",
    "    test_end = time()\n",
    "    \n",
    "    return [(train_end - train_start), (test_end - train_end)] + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "155/155 [==============================] - 25s 158ms/step - loss: 1.8756 - crf_viterbi_accuracy: 0.3617 - val_loss: 1.4914 - val_crf_viterbi_accuracy: 0.5116\n",
      "Epoch 2/25\n",
      "155/155 [==============================] - 15s 94ms/step - loss: 1.4077 - crf_viterbi_accuracy: 0.5275 - val_loss: 1.3243 - val_crf_viterbi_accuracy: 0.5549\n",
      "Epoch 3/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 1.2898 - crf_viterbi_accuracy: 0.5643 - val_loss: 1.2491 - val_crf_viterbi_accuracy: 0.5761\n",
      "Epoch 4/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 1.2158 - crf_viterbi_accuracy: 0.5788 - val_loss: 1.2814 - val_crf_viterbi_accuracy: 0.5192\n",
      "Epoch 5/25\n",
      "155/155 [==============================] - 15s 94ms/step - loss: 1.1625 - crf_viterbi_accuracy: 0.5887 - val_loss: 1.1560 - val_crf_viterbi_accuracy: 0.5910\n",
      "Epoch 6/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 1.1109 - crf_viterbi_accuracy: 0.6003 - val_loss: 1.1297 - val_crf_viterbi_accuracy: 0.5961\n",
      "Epoch 7/25\n",
      "155/155 [==============================] - 14s 92ms/step - loss: 1.0665 - crf_viterbi_accuracy: 0.6111 - val_loss: 1.1007 - val_crf_viterbi_accuracy: 0.6048\n",
      "Epoch 8/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 1.0256 - crf_viterbi_accuracy: 0.6207 - val_loss: 1.0833 - val_crf_viterbi_accuracy: 0.6107\n",
      "Epoch 9/25\n",
      "155/155 [==============================] - 14s 92ms/step - loss: 0.9873 - crf_viterbi_accuracy: 0.6326 - val_loss: 1.0511 - val_crf_viterbi_accuracy: 0.6100\n",
      "Epoch 10/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.9479 - crf_viterbi_accuracy: 0.6436 - val_loss: 1.0491 - val_crf_viterbi_accuracy: 0.6002\n",
      "Epoch 11/25\n",
      "155/155 [==============================] - 15s 94ms/step - loss: 0.9119 - crf_viterbi_accuracy: 0.6514 - val_loss: 1.0199 - val_crf_viterbi_accuracy: 0.6193\n",
      "Epoch 12/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.8747 - crf_viterbi_accuracy: 0.6632 - val_loss: 1.0196 - val_crf_viterbi_accuracy: 0.6096\n",
      "Epoch 13/25\n",
      "155/155 [==============================] - 14s 92ms/step - loss: 0.8401 - crf_viterbi_accuracy: 0.6714 - val_loss: 1.0077 - val_crf_viterbi_accuracy: 0.6232\n",
      "Epoch 14/25\n",
      "155/155 [==============================] - 14s 94ms/step - loss: 0.8059 - crf_viterbi_accuracy: 0.6824 - val_loss: 0.9980 - val_crf_viterbi_accuracy: 0.6229\n",
      "Epoch 15/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.7735 - crf_viterbi_accuracy: 0.6917 - val_loss: 0.9890 - val_crf_viterbi_accuracy: 0.6279\n",
      "Epoch 16/25\n",
      "155/155 [==============================] - 14s 92ms/step - loss: 0.7457 - crf_viterbi_accuracy: 0.7009 - val_loss: 0.9841 - val_crf_viterbi_accuracy: 0.6281\n",
      "Epoch 17/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.7171 - crf_viterbi_accuracy: 0.7061 - val_loss: 0.9944 - val_crf_viterbi_accuracy: 0.6207\n",
      "Epoch 18/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.6877 - crf_viterbi_accuracy: 0.7172 - val_loss: 0.9883 - val_crf_viterbi_accuracy: 0.6298\n",
      "Epoch 19/25\n",
      "155/155 [==============================] - 14s 92ms/step - loss: 0.6629 - crf_viterbi_accuracy: 0.7230 - val_loss: 0.9944 - val_crf_viterbi_accuracy: 0.6239\n",
      "Epoch 20/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.6366 - crf_viterbi_accuracy: 0.7299 - val_loss: 1.0007 - val_crf_viterbi_accuracy: 0.6242\n",
      "Epoch 21/25\n",
      "155/155 [==============================] - 14s 93ms/step - loss: 0.6122 - crf_viterbi_accuracy: 0.7382 - val_loss: 0.9981 - val_crf_viterbi_accuracy: 0.6273\n"
     ]
    }
   ],
   "source": [
    "lstm_crf = run_lstmcrf_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes, just because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(models)\n",
    "def mnb():\n",
    "    mnb = models.NaiveBayes(corpus, ngram_range = (1,2), tfidf = True)\n",
    "    \n",
    "    train_start = time()\n",
    "    \n",
    "    mnb.train()\n",
    "    \n",
    "    train_end = time()\n",
    "    \n",
    "    acc = mnb.eval_on_test()\n",
    "    \n",
    "    test_end = time()\n",
    "    \n",
    "    return [(train_end - train_start), (test_end - train_end), np.nan, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_results = mnb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LSTM-Softmax</th>\n",
       "      <td>223.217211</td>\n",
       "      <td>1.126713</td>\n",
       "      <td>1.031624</td>\n",
       "      <td>0.645404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN</th>\n",
       "      <td>70.461788</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>1.081208</td>\n",
       "      <td>0.625740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM-CRF</th>\n",
       "      <td>319.369356</td>\n",
       "      <td>1.696913</td>\n",
       "      <td>1.009526</td>\n",
       "      <td>0.623276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.475121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1         2         3\n",
       "LSTM-Softmax  223.217211  1.126713  1.031624  0.645404\n",
       "CNN            70.461788  0.338728  1.081208  0.625740\n",
       "LSTM-CRF      319.369356  1.696913  1.009526  0.623276\n",
       "Naive Bayes     0.065760  0.015556       NaN  0.475121"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.DataFrame({\"LSTM-Softmax\":lstm_results, \"CNN\":cnn_results, \"LSTM-CRF\":lstm_crf,\n",
    "              \"Naive Bayes\":nb_results}).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
