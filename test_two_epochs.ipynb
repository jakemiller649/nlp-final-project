{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we let these guys, let's test a couple epochs just to get a baseline ...\n",
    "\n",
    "these are on 1 CPU machines ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "import models_tf_only as models\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger # maybe more\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CNN_model():\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 25\n",
    "\n",
    "    # choose hyperparameters\n",
    "    d1 = np.random.randint(0,3)\n",
    "    d2 = np.random.randint(0,3)\n",
    "    filters = np.random.randint(50,1001)\n",
    "    kernel_size = np.random.randint(1,11)\n",
    "    hidden_units = np.random.randint(25,251)\n",
    "    dropout_rate = round(np.random.uniform(0,1,size=None), 3)\n",
    "    trainable_embed = True\n",
    "    embed_vec = np.random.choice(['glove200', 'glove300', 'numberbatch'])\n",
    " \n",
    "\n",
    "\n",
    "    ## start the timer\n",
    "    start = time()\n",
    "\n",
    "    # load the corpus\n",
    "    corpus = utils.AMI_Corpus(seed = 75, embed_vec = embed_vec)\n",
    "\n",
    "    cnn = models.CNN(corpus = corpus, d1 = d1, d2 = d2,\n",
    "                       batch_size = BATCH_SIZE,\n",
    "                       filters = filters,\n",
    "                       kernel_size = kernel_size,\n",
    "                       hidden_units = hidden_units,\n",
    "                       dropout_rate = dropout_rate,\n",
    "                       trainable_embed = trainable_embed)\n",
    "\n",
    "    cnn.model.compile(optimizer = 'adagrad', metrics = ['acc'], loss = 'categorical_crossentropy')\n",
    "\n",
    "    # create our generators\n",
    "    ug_train = utils.UtteranceGenerator(corpus, \"train\", batch_size = BATCH_SIZE, sequence_length = (d1 + d2 + 1), conv = True)\n",
    "    ug_val = utils.UtteranceGenerator(corpus, \"val\", batch_size = BATCH_SIZE, sequence_length = (d1 + d2 + 1), conv = True)\n",
    "\n",
    "    # create keras callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "    right_now = datetime.now().isoformat() # timestamp\n",
    "    csv_logger = CSVLogger('logs/cnn_history_' + right_now + \".csv\") # log epochs in case I want to look back later\n",
    "\n",
    "    \n",
    "    history = cnn.model.fit_generator(ug_train, epochs=EPOCHS, verbose=1, callbacks=[es, csv_logger],\n",
    "                                          validation_data=ug_val, #validation_freq=1,\n",
    "                                          use_multiprocessing=False, shuffle=True)\n",
    "\n",
    "\n",
    "    results = {\"d1\":d1, \"d2\":d2, \"filters\":filters, \"kernel_size\":kernel_size, \"hidden_units\":hidden_units, \"dropout_rate\":dropout_rate,\n",
    "               \"embed_vec\":embed_vec, \"acc\":history.history['acc'][-5:], \"val_acc\":history.history['acc'][-5:],\n",
    "               \"loss\":history.history['loss'][-5:], \"val_loss\":history.history['val_loss'][-5:], \"time\":(time() - start), \n",
    "               \"trainable_embed\":trainable_embed}\n",
    "\n",
    "    results = {str(k):str(v) for k,v in results.items()}\n",
    "    with open(\"logs/cnn_params_\" + right_now + \".json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from /home/jake_miller/final/nlp-final-project/data\n",
      "Begin corpus post-processing ...\n",
      "Splitting corpus into training and test ...\n",
      "Creating vocabulary from training set ...\n",
      "Found 8435 unique words.\n",
      "Building initial embedding matrix ...\n",
      "(8437, 300)\n",
      "loading pretrained vectors from glove.6B.300d.txt\n",
      "Epoch 1/25\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 1.2022 - acc: 0.6027\n",
      "156/156 [==============================] - 5s 34ms/step - loss: 1.6805 - acc: 0.5323 - val_loss: 1.2022 - val_acc: 0.6027\n",
      "Epoch 2/25\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.1427 - acc: 0.6202\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 1.1601 - acc: 0.6097 - val_loss: 1.1427 - val_acc: 0.6202\n",
      "Epoch 3/25\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.1105 - acc: 0.6280\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 1.0945 - acc: 0.6287 - val_loss: 1.1105 - val_acc: 0.6280\n",
      "Epoch 4/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0928 - acc: 0.6343\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 1.0532 - acc: 0.6401 - val_loss: 1.0928 - val_acc: 0.6343\n",
      "Epoch 5/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0888 - acc: 0.6317\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 1.0226 - acc: 0.6510 - val_loss: 1.0888 - val_acc: 0.6317\n",
      "Epoch 6/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0743 - acc: 0.6398\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.9944 - acc: 0.6598 - val_loss: 1.0743 - val_acc: 0.6398\n",
      "Epoch 7/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0736 - acc: 0.6361\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.9706 - acc: 0.6677 - val_loss: 1.0736 - val_acc: 0.6361\n",
      "Epoch 8/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0660 - acc: 0.6378\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.9508 - acc: 0.6749 - val_loss: 1.0660 - val_acc: 0.6378\n",
      "Epoch 9/25\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0757 - acc: 0.6324ETA: 0s - loss: 0.932\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.9304 - acc: 0.6814 - val_loss: 1.0757 - val_acc: 0.6324\n",
      "Epoch 10/25\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0748 - acc: 0.6342\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.9123 - acc: 0.6898 - val_loss: 1.0748 - val_acc: 0.6342\n",
      "Epoch 11/25\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 1.1542 - acc: 0.5961\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.8918 - acc: 0.6974 - val_loss: 1.1542 - val_acc: 0.5961\n",
      "Epoch 12/25\n",
      "26/26 [==============================] - 0s 7ms/step - loss: 1.0809 - acc: 0.6334\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.8758 - acc: 0.7004 - val_loss: 1.0809 - val_acc: 0.6334\n",
      "Epoch 13/25\n",
      "26/26 [==============================] - 0s 6ms/step - loss: 1.0903 - acc: 0.6274\n",
      "156/156 [==============================] - 4s 24ms/step - loss: 0.8577 - acc: 0.7090 - val_loss: 1.0903 - val_acc: 0.6274\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(utils)\n",
    "reload(models)\n",
    "results = run_CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1': '2',\n",
       " 'd2': '2',\n",
       " 'filters': '910',\n",
       " 'kernel_size': '8',\n",
       " 'hidden_units': '186',\n",
       " 'dropout_rate': '0.552',\n",
       " 'embed_vec': 'glove200',\n",
       " 'acc': '[0.27962798, 0.27962798, 0.27962798, 0.27962798, 0.27962798]',\n",
       " 'val_acc': '[0.27962798, 0.27962798, 0.27962798, 0.27962798, 0.27962798]',\n",
       " 'loss': '[11.611024901525068, 11.611024901525068, 11.611024901525068, 11.611024901525068, 11.611024901525068]',\n",
       " 'val_loss': '[11.429241767296425, 11.429241767296425, 11.429241767296425, 11.429241767296425, 11.429241767296425]',\n",
       " 'time': '78.0381224155426'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {str(k):str(v) for k,v in resutls.items()}\n",
    "\n",
    "with open(\"logs/TEST_gpu_cnn_params_.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on one CPU, with a batch size of 25, we were looking at around 37 (!) minutes for one f'cking epoch (regardless, it seems to be running, so I'm killing it. We'll run them for real with a bigger machine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's test the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_model():\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 10\n",
    "\n",
    "    # choose hyperparameters\n",
    "    num_layers = np.random.randint(2,16) # lstm layers\n",
    "    hidden_state_size = np.random.randint(100,400)\n",
    "    dropout_rate = round(np.random.uniform(0,1,size=None), 3)\n",
    "    trainable_embed = True\n",
    "    embed_vec = np.random.choice(['glove200', 'glove300', 'numberbatch'])\n",
    "    stateful = False\n",
    "    bidirectional = True\n",
    "\n",
    "\n",
    "    ## start the timer\n",
    "    start = time()\n",
    "\n",
    "    # load the corpus\n",
    "    corpus = utils.AMI_Corpus(seed = 75, embed_vec = embed_vec)\n",
    "\n",
    "    lstm = models.LSTMSoftmax(corpus,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              num_layers = num_layers,\n",
    "                              dropout_rate = dropout_rate,\n",
    "                              hidden_state_size = hidden_state_size,\n",
    "                              stateful = stateful, bidirectional = bidirectional, trainable_embed = trainable_embed)\n",
    "\n",
    "    lstm.model.compile(optimizer = 'adagrad', metrics = ['acc'], loss = 'categorical_crossentropy')\n",
    "\n",
    "    # create our generators\n",
    "    ug_train = utils.UtteranceGenerator(corpus, \"train\", batch_size = BATCH_SIZE)\n",
    "    ug_val = utils.UtteranceGenerator(corpus, \"val\", batch_size = BATCH_SIZE)\n",
    "\n",
    "    # create keras callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "    right_now = datetime.now().isoformat() # timestamp\n",
    "    csv_logger = CSVLogger('logs/lstm_history_' + right_now) # log epochs in case I want to look back later\n",
    "\n",
    "    # note to self, maybe change validation_steps and validation_freq\n",
    "    history = lstm.model.fit_generator(ug_train, epochs=EPOCHS, verbose=1, callbacks=[es, csv_logger],\n",
    "                      validation_data=ug_val, #validation_steps=100, #validation_freq=1,\n",
    "                      use_multiprocessing=False, shuffle=True)\n",
    "\n",
    "    results = {\"num_layers\":num_layers, \"hidden_state_size\":hidden_state_size,\"dropout_rate\":dropout_rate,\n",
    "               \"embed_vec\":embed_vec, \"acc\":history.history['acc'][-5:], \"val_acc\":history.history['acc'][-5:],\n",
    "               \"loss\":history.history['loss'][-5:], \"val_loss\":history.history['val_loss'][-5:], \"time\":time() - start,\n",
    "                \"trainable_embed\":trainable_embed, \"stateful\":stateful, \"bidirectional\":bidirectional}\n",
    "    \n",
    "    results = {str(k):str(v) for k,v in results.items()}\n",
    "\n",
    "    with open(\"logs/lstm_params_\" + right_now + \".json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "        \n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna need a bigger boat! This one estimates at roughly an hour per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from /home/jake_miller/final/nlp-final-project/data\n",
      "Begin corpus post-processing ...\n",
      "Splitting corpus into training and test ...\n",
      "Creating vocabulary from training set ...\n",
      "Found 8435 unique words.\n",
      "Building initial embedding matrix ...\n",
      "(8437, 300)\n",
      "loading pretrained vectors from glove.6B.300d.txt\n",
      "Epoch 1/10\n",
      "130/130 [==============================] - 5s 39ms/step - loss: 1.5821 - acc: 0.4988\n",
      "798/798 [==============================] - 69s 87ms/step - loss: 2.0055 - acc: 0.3161 - val_loss: 1.5821 - val_acc: 0.4988\n",
      "Epoch 2/10\n",
      "130/130 [==============================] - 4s 28ms/step - loss: 1.5018 - acc: 0.5224\n",
      "798/798 [==============================] - 64s 80ms/step - loss: 1.5874 - acc: 0.4799 - val_loss: 1.5018 - val_acc: 0.5224\n",
      "Epoch 3/10\n",
      "130/130 [==============================] - 4s 27ms/step - loss: 1.4259 - acc: 0.5494\n",
      "798/798 [==============================] - 64s 80ms/step - loss: 1.4877 - acc: 0.5173 - val_loss: 1.4259 - val_acc: 0.5494\n",
      "Epoch 4/10\n",
      "130/130 [==============================] - 4s 28ms/step - loss: 1.3511 - acc: 0.5701\n",
      "798/798 [==============================] - 63s 79ms/step - loss: 1.4188 - acc: 0.5391 - val_loss: 1.3511 - val_acc: 0.5701\n",
      "Epoch 5/10\n",
      "130/130 [==============================] - 4s 28ms/step - loss: 1.3217 - acc: 0.5800\n",
      "798/798 [==============================] - 64s 80ms/step - loss: 1.3612 - acc: 0.5568 - val_loss: 1.3217 - val_acc: 0.5800\n",
      "Epoch 6/10\n",
      "130/130 [==============================] - 4s 28ms/step - loss: 1.2973 - acc: 0.5816\n",
      "798/798 [==============================] - 64s 80ms/step - loss: 1.3286 - acc: 0.5673 - val_loss: 1.2973 - val_acc: 0.5816\n",
      "Epoch 7/10\n",
      "130/130 [==============================] - 4s 27ms/step - loss: 1.2771 - acc: 0.5855\n",
      "798/798 [==============================] - 63s 79ms/step - loss: 1.3088 - acc: 0.5717 - val_loss: 1.2771 - val_acc: 0.5855\n",
      "Epoch 8/10\n",
      "130/130 [==============================] - 4s 27ms/step - loss: 1.2677 - acc: 0.5875\n",
      "798/798 [==============================] - 63s 80ms/step - loss: 1.2929 - acc: 0.5749 - val_loss: 1.2677 - val_acc: 0.5875\n",
      "Epoch 9/10\n",
      "130/130 [==============================] - 4s 27ms/step - loss: 1.2509 - acc: 0.5928\n",
      "798/798 [==============================] - 63s 80ms/step - loss: 1.2774 - acc: 0.5784 - val_loss: 1.2509 - val_acc: 0.5928\n",
      "Epoch 10/10\n",
      "130/130 [==============================] - 4s 28ms/step - loss: 1.2407 - acc: 0.5966\n",
      "798/798 [==============================] - 64s 80ms/step - loss: 1.2634 - acc: 0.5825 - val_loss: 1.2407 - val_acc: 0.5966\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "reload(utils)\n",
    "reload(models)\n",
    "reults = run_lstm_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self:\n",
    "\n",
    "```\n",
    "ValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n",
    "- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\n",
    "- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = np.random.choice([True, False], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_embed = np.random.choice([True, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = np.random.randint(2,16) # lstm layers\n",
    "hidden_state_size = np.random.randint(100,400)\n",
    "dropout_rate = round(np.random.uniform(0,1,size=None), 3)\n",
    "embed_vec = np.random.choice(['glove200', 'glove300', 'numberbatch'])\n",
    "bidirectional = np.random.choice([True, False])\n",
    "stateful = False\n",
    "trainable_embed = np.random.choice([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
